{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Import to list directories \n",
      "import os\n",
      "\n",
      "print(os.getcwd())\n",
      "print(os.listdir(os.getcwd()))\n",
      "\n",
      "### import for time and time monitoring \n",
      "import time\n",
      "## Imports for Metrics operations \n",
      "import numpy as np \n",
      "## Imports for dataframes and .csv managment\n",
      "import pandas as pd \n",
      "\n",
      "# TensorFlow library  Imports\n",
      "import tensorflow as tf\n",
      "print(\"tf version: \", tf.__version__)\n",
      "\n",
      "\n",
      "\n",
      "import swish_package\n",
      "from swish_package import swish\n",
      "\n",
      "\n",
      "\n",
      "# Keras (backended with Tensorflow) Imports\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import backend as K\n",
      "from tensorflow.keras.models import Sequential, Model\n",
      "from tensorflow.keras.layers import Dense, Dropout, Input\n",
      "from tensorflow.keras.optimizers import Adam\n",
      "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
      "from tensorflow.keras.callbacks import CSVLogger\n",
      "\n",
      "# Sklearn package for machine learining models \n",
      "## WILL BE USED TO SPLIT  TRAIN_VAL DATASETS\n",
      "from sklearn import metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Garbage Collectors\n",
      "import gc\n",
      "import sys\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Import Transformers to get Tokenizer for bert and bert models\n",
      "\n",
      "import transformers\n",
      "\n",
      "from transformers import DistilBertTokenizer, TFDistilBertModel , RobertaTokenizer\n",
      "from tqdm.notebook import tqdm\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
      "from transformers import XLMRobertaTokenizer ,TFRobertaModel , TFAutoModel\n",
      "from transformers import XLMRobertaForSequenceClassification\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def regular_encode(texts, tokenizer, maxlen=512):\n",
      "    enc_di = tokenizer.batch_encode_plus(\n",
      "        texts, \n",
      "        return_attention_masks=False, \n",
      "        return_token_type_ids=False,\n",
      "        pad_to_max_length=True,\n",
      "        max_length=maxlen\n",
      "    )\n",
      "    return np.array(enc_di['input_ids'])\n",
      "\n",
      "\n",
      "def build_model(transformer, max_len=512):\n",
      "\n",
      "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
      "    sequence_output = transformer(input_word_ids)[0]\n",
      "    cls_token = sequence_output[:, 0, :]\n",
      "    out = Dense(1, activation='sigmoid')(cls_token)\n",
      "    \n",
      "    model = Model(inputs=input_word_ids, outputs=out)\n",
      "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    \n",
      "    return model\n",
      "\n",
      "\n",
      "def get_train_data():\n",
      "    train = pd.read_csv('/content/jigsaw_mjy_train_val_openaug_523200.csv')\n",
      "    #test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
      "    return train\n",
      "####\n",
      "#\n",
      "# Prediction fn --> SageMaker\n",
      "####\n",
      "\n",
      "\n",
      "def predict_fn(input_data, model):\n",
      "    MAX_LEN = 192\n",
      "    MODEL = 'distilbert-base-multilingual-cased'\n",
      "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL)\n",
      "    x_test = regular_encode(input_data, tokenizer, maxlen=MAX_LEN)\n",
      "    pred = model.predict(x_test)\n",
      "    return pred"
     ]
    }
   ],
   "source": [
    "!cat entry_point.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('1', recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='modell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/modell/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '2.3',\n",
    "                                  entry_point = 'inference.py',\n",
    "                                  source_dir='my_src',\n",
    "                                  env={'SAGEMAKER_REQUIREMENTS': 'requirements.txt'}                                           \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!CPU times: user 1min 28s, sys: 12.6 s, total: 1min 41s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1 , instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.10\n",
      "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
      "\u001b[K     |████████████████████████████████| 660 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (1.19.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (4.42.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (3.0.12)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 20.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 63.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (2020.12.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 69.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers==2.10) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers==2.10) (7.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers==2.10) (0.14.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 65.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=58ce18c4490a1dd9c04383d1d94f66e079945ab3594c699deeeae94b3fe56b04\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, tokenizers, sentencepiece, sacremoses, dataclasses, transformers\n",
      "Successfully installed dataclasses-0.8 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.7.0 transformers-2.10.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_tf_distilbert import TFDistilBertModel\n",
    "# TensorFlow library  Imports\n",
    "import tensorflow as tf\n",
    "# Keras (backended with Tensorflow) Imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696803f562464d009c8d4b560b86a6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "# Configuration\n",
    "MAX_LEN = 192\n",
    "#MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "#MODEL = 'roberta-base'\n",
    "MODEL = 'distilbert-base-multilingual-cased'\n",
    "EPOCHS = 10\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "D = np.array([\"I want to see you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = regular_encode(D, tokenizer, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,   146, 21528, 10114, 12888, 13028, 13935,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Tu devrais mourir bientôt\"\n",
    "pred = predictor.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.953224897]]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pred['predictions'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
